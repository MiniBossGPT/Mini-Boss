{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Mini-Boss Welcome to Mini-Boss. Please follow the Installation guide to get started. It is recommended to use a virtual machine for tasks that require high security measures to prevent any potential harm to the main computer's system and data.","title":"Home"},{"location":"#mini-boss","text":"Welcome to Mini-Boss. Please follow the Installation guide to get started. It is recommended to use a virtual machine for tasks that require high security measures to prevent any potential harm to the main computer's system and data.","title":"Mini-Boss"},{"location":"plugins/","text":"Use Auto-GPT Plugins with Mini-Boss \u26a0\ufe0f\ud83d\udc80 WARNING \ud83d\udc80\u26a0\ufe0f: Review the code of any plugin you use thoroughly, as plugins can execute any Python code, potentially leading to malicious activities, such as stealing your API keys. See the Auto-GPT plugin repo Plugins Repo for more info on how to install all the amazing plugins the community has built! Alternatively, developers can use the Auto-GPT Plugin Template as a starting point for creating your own plugins.","title":"Plugins"},{"location":"plugins/#use-auto-gpt-plugins-with-mini-boss","text":"\u26a0\ufe0f\ud83d\udc80 WARNING \ud83d\udc80\u26a0\ufe0f: Review the code of any plugin you use thoroughly, as plugins can execute any Python code, potentially leading to malicious activities, such as stealing your API keys. See the Auto-GPT plugin repo Plugins Repo for more info on how to install all the amazing plugins the community has built! Alternatively, developers can use the Auto-GPT Plugin Template as a starting point for creating your own plugins.","title":"Use Auto-GPT Plugins with Mini-Boss"},{"location":"setup/","text":"Setting up Mini-Boss \ud83d\udccb Requirements Choose an environment to run Mini-Boss in (pick one): Docker ( recommended ) Python 3.10 or later (instructions: for Windows ) VSCode + devcontainer \ud83d\udddd\ufe0f Getting an API key Get your OpenAI API key from: https://platform.openai.com/account/api-keys . Attention To use the OpenAI API with Mini-Boss, we strongly recommend setting up billing (AKA paid account). Free accounts are limited to 3 API calls per minute, which can cause the application to crash. You can set up a paid account at Manage account > Billing > Overview . Important It's highly recommended that you keep keep track of your API costs on the Usage page . You can also set limits on how much you spend on the Usage limits page . Setting up Mini-Boss Set up with Docker Make sure you have Docker installed, see requirements Pull the latest image from Docker Hub docker pull minibossgpt/mini-boss Create a folder for Mini-Boss In the folder, create a file called docker-compose.yml with the following contents: version : \"3.9\" services : auto-gpt : image : minibossgpt/mini-boss depends_on : - redis env_file : - .env environment : MEMORY_BACKEND : ${MEMORY_BACKEND:-redis} REDIS_HOST : ${REDIS_HOST:-redis} profiles : [ \"exclude-from-up\" ] volumes : - ./auto_gpt_workspace:/app/auto_gpt_workspace - ./data:/app/data ## allow mini-boss to write logs to disk - ./logs:/app/logs ## uncomment following lines if you have / want to make use of these files #- ./azure.yaml:/app/azure.yaml #- ./ai_settings.yaml:/app/ai_settings.yaml redis : image : \"redis/redis-stack-server:latest\" Create the necessary configuration files. If needed, you can find templates in the repository . Continue to Run with Docker Docker only supports headless browsing Mini-Boss uses a browser in headless mode by default: HEADLESS_BROWSER=True . Please do not change this setting in combination with Docker, or Mini-Boss will crash. Set up with Git Important Make sure you have Git installed for your OS. Executing commands To execute the given commands, open a CMD, Bash, or Powershell window. On Windows: press Win + X and pick Terminal , or Win + R and enter cmd Clone the repository git clone -b stable https://github.com/MiniBossGPT/Mini-Boss.git Navigate to the directory where you downloaded the repository cd Mini-Boss Set up without Git/Docker Warning We recommend to use Git or Docker, to make updating easier. Download Source code (zip) from the latest stable release Extract the zip-file into a folder Configuration (follow guide for Auto-GPT) Find the file named auto-gpt/.env.template in the main Auto-GPT folder. This file may be hidden by default in some operating systems due to the dot prefix. To reveal hidden files, follow the instructions for your specific operating system: [Windows][show hidden files/Windows], [macOS][show hidden files/macOS]. Either run ./run_scripts/setup.sh or create a copy of .env.template and call it .env ; if you're already in a command prompt/terminal window: cp auto-gpt/.env.template auto-gpt/.env . Open the .env file in both the top level directoy and the sub directory in a text editor. Find the line that says OPENAI_API_KEY= . After the = , enter your unique OpenAI API Key without any quotes or spaces . Enter any other API keys or tokens for services you would like to use. Note To activate and adjust a setting, remove the # prefix. Save and close the .env file. Now copy the file to the top level. cp auto-gpt/.env .env Running Mini-Boss Run with Docker Easiest is to use docker-compose . Run the commands below in your Mini-Boss folder. Build the image. If you have pulled the image from Docker Hub, skip this step. docker-compose build mini-boss Run Mini-Boss docker-compose run --rm mini-boss By default, this will also start and attach a Redis memory backend. If you do not want this, comment or remove the depends: - redis and redis: sections from docker-compose.yml . For related settings, see Memory > Redis setup . You can pass extra arguments, e.g. running with --gpt3only and --continuous : docker-compose run --rm mini-boss --gpt3only --continuous If you dare, you can also build and run it with \"vanilla\" docker commands: docker build -t mini-boss . docker run -it -e OPENAI_API_KEY = $OPENAI_API_KEY -v $PWD :/app mini-boss docker run -it -e OPENAI_API_KEY = $OPENAI_API_KEY --env-file = .env -v $PWD :/app mini-boss docker run -it --env-file = .env -v $PWD :/app --rm mini-boss --gpt3only --continuous Run with Dev Container Install the Remote - Containers extension in VS Code. Open command palette with F1 and type Dev Containers: Open Folder in Container . Run ./run_scripts/run.sh . Run without Docker Simply run the startup script in your terminal. This will install any necessary Python packages and launch Mini-Boss. On Linux/MacOS: ./run_scripts/run.sh On Windows: . \\r un_scripts \\r un.bat If this gives errors, make sure you have a compatible Python version installed. See also the requirements .","title":"Setup"},{"location":"setup/#setting-up-mini-boss","text":"","title":"Setting up Mini-Boss"},{"location":"setup/#requirements","text":"Choose an environment to run Mini-Boss in (pick one): Docker ( recommended ) Python 3.10 or later (instructions: for Windows ) VSCode + devcontainer","title":"\ud83d\udccb Requirements"},{"location":"setup/#getting-an-api-key","text":"Get your OpenAI API key from: https://platform.openai.com/account/api-keys . Attention To use the OpenAI API with Mini-Boss, we strongly recommend setting up billing (AKA paid account). Free accounts are limited to 3 API calls per minute, which can cause the application to crash. You can set up a paid account at Manage account > Billing > Overview . Important It's highly recommended that you keep keep track of your API costs on the Usage page . You can also set limits on how much you spend on the Usage limits page .","title":"\ud83d\udddd\ufe0f Getting an API key"},{"location":"setup/#setting-up-mini-boss_1","text":"","title":"Setting up Mini-Boss"},{"location":"setup/#set-up-with-docker","text":"Make sure you have Docker installed, see requirements Pull the latest image from Docker Hub docker pull minibossgpt/mini-boss Create a folder for Mini-Boss In the folder, create a file called docker-compose.yml with the following contents: version : \"3.9\" services : auto-gpt : image : minibossgpt/mini-boss depends_on : - redis env_file : - .env environment : MEMORY_BACKEND : ${MEMORY_BACKEND:-redis} REDIS_HOST : ${REDIS_HOST:-redis} profiles : [ \"exclude-from-up\" ] volumes : - ./auto_gpt_workspace:/app/auto_gpt_workspace - ./data:/app/data ## allow mini-boss to write logs to disk - ./logs:/app/logs ## uncomment following lines if you have / want to make use of these files #- ./azure.yaml:/app/azure.yaml #- ./ai_settings.yaml:/app/ai_settings.yaml redis : image : \"redis/redis-stack-server:latest\" Create the necessary configuration files. If needed, you can find templates in the repository . Continue to Run with Docker Docker only supports headless browsing Mini-Boss uses a browser in headless mode by default: HEADLESS_BROWSER=True . Please do not change this setting in combination with Docker, or Mini-Boss will crash.","title":"Set up with Docker"},{"location":"setup/#set-up-with-git","text":"Important Make sure you have Git installed for your OS. Executing commands To execute the given commands, open a CMD, Bash, or Powershell window. On Windows: press Win + X and pick Terminal , or Win + R and enter cmd Clone the repository git clone -b stable https://github.com/MiniBossGPT/Mini-Boss.git Navigate to the directory where you downloaded the repository cd Mini-Boss","title":"Set up with Git"},{"location":"setup/#set-up-without-gitdocker","text":"Warning We recommend to use Git or Docker, to make updating easier. Download Source code (zip) from the latest stable release Extract the zip-file into a folder","title":"Set up without Git/Docker"},{"location":"setup/#configuration-follow-guide-for-auto-gpt","text":"Find the file named auto-gpt/.env.template in the main Auto-GPT folder. This file may be hidden by default in some operating systems due to the dot prefix. To reveal hidden files, follow the instructions for your specific operating system: [Windows][show hidden files/Windows], [macOS][show hidden files/macOS]. Either run ./run_scripts/setup.sh or create a copy of .env.template and call it .env ; if you're already in a command prompt/terminal window: cp auto-gpt/.env.template auto-gpt/.env . Open the .env file in both the top level directoy and the sub directory in a text editor. Find the line that says OPENAI_API_KEY= . After the = , enter your unique OpenAI API Key without any quotes or spaces . Enter any other API keys or tokens for services you would like to use. Note To activate and adjust a setting, remove the # prefix. Save and close the .env file. Now copy the file to the top level. cp auto-gpt/.env .env","title":"Configuration (follow guide for Auto-GPT)"},{"location":"setup/#running-mini-boss","text":"","title":"Running Mini-Boss"},{"location":"setup/#run-with-docker","text":"Easiest is to use docker-compose . Run the commands below in your Mini-Boss folder. Build the image. If you have pulled the image from Docker Hub, skip this step. docker-compose build mini-boss Run Mini-Boss docker-compose run --rm mini-boss By default, this will also start and attach a Redis memory backend. If you do not want this, comment or remove the depends: - redis and redis: sections from docker-compose.yml . For related settings, see Memory > Redis setup . You can pass extra arguments, e.g. running with --gpt3only and --continuous : docker-compose run --rm mini-boss --gpt3only --continuous If you dare, you can also build and run it with \"vanilla\" docker commands: docker build -t mini-boss . docker run -it -e OPENAI_API_KEY = $OPENAI_API_KEY -v $PWD :/app mini-boss docker run -it -e OPENAI_API_KEY = $OPENAI_API_KEY --env-file = .env -v $PWD :/app mini-boss docker run -it --env-file = .env -v $PWD :/app --rm mini-boss --gpt3only --continuous","title":"Run with Docker"},{"location":"setup/#run-with-dev-container","text":"Install the Remote - Containers extension in VS Code. Open command palette with F1 and type Dev Containers: Open Folder in Container . Run ./run_scripts/run.sh .","title":"Run with Dev Container"},{"location":"setup/#run-without-docker","text":"Simply run the startup script in your terminal. This will install any necessary Python packages and launch Mini-Boss. On Linux/MacOS: ./run_scripts/run.sh On Windows: . \\r un_scripts \\r un.bat If this gives errors, make sure you have a compatible Python version installed. See also the requirements .","title":"Run without Docker"},{"location":"usage/","text":"Usage Command Line Arguments Running with --help lists all the possible command line arguments you can pass: ./run.sh --help # on Linux / macOS . \\r un.bat --help # on Windows Info For use with Docker, replace the script in the examples with docker-compose run --rm auto-gpt : docker-compose run --rm auto-gpt --help docker-compose run --rm auto-gpt --ai-settings <filename> Note Replace anything in angled brackets (<>) to a value you want to specify Here are some common arguments you can use when running Mini-Boss: Run Mini-Boss with a different AI Settings file ./run.sh --ai-settings <filename> Specify a memory backend ./run.sh --use-memory <memory-backend> Note There are shorthands for some of these flags, for example -m for --use-memory . Use ./run.sh --help for more information. Speak Mode Enter this command to use TTS (Text-to-Speech) for Mini-Boss ./run.sh --speak \ud83d\udc80 Continuous Mode \u26a0\ufe0f Run the AI without user authorization, 100% automated. Continuous mode is NOT recommended. It is potentially dangerous and may cause your AI to run forever or carry out actions you would not usually authorize. Use at your own risk. ./run.sh --continuous To exit the program, press Ctrl + C \u267b\ufe0f Self-Feedback Mode \u26a0\ufe0f Running Self-Feedback will INCREASE token use and thus cost more. This feature enables the agent to provide self-feedback by verifying its own actions and checking if they align with its current goals. If not, it will provide better feedback for the next loop. To enable this feature for the current loop, input S into the input field. GPT-3.5 ONLY Mode If you don't have access to GPT-4, this mode allows you to use Mini-Boss! ./run.sh --gpt3only You can achieve the same by setting SMART_LLM_MODEL in .env to gpt-3.5-turbo . GPT-4 ONLY Mode If you have access to GPT-4, this mode allows you to use Mini-Boss solely with GPT-4. This may give your bot increased intelligence. ./run.sh --gpt4only Warning Since GPT-4 is more expensive to use, running Mini-Boss in GPT-4-only mode will increase your API costs. Logs Activity and error logs are located in the ./output/logs To print out debug logs: ./run.sh --debug","title":"Usage"},{"location":"usage/#usage","text":"","title":"Usage"},{"location":"usage/#command-line-arguments","text":"Running with --help lists all the possible command line arguments you can pass: ./run.sh --help # on Linux / macOS . \\r un.bat --help # on Windows Info For use with Docker, replace the script in the examples with docker-compose run --rm auto-gpt : docker-compose run --rm auto-gpt --help docker-compose run --rm auto-gpt --ai-settings <filename> Note Replace anything in angled brackets (<>) to a value you want to specify Here are some common arguments you can use when running Mini-Boss: Run Mini-Boss with a different AI Settings file ./run.sh --ai-settings <filename> Specify a memory backend ./run.sh --use-memory <memory-backend> Note There are shorthands for some of these flags, for example -m for --use-memory . Use ./run.sh --help for more information.","title":"Command Line Arguments"},{"location":"usage/#speak-mode","text":"Enter this command to use TTS (Text-to-Speech) for Mini-Boss ./run.sh --speak","title":"Speak Mode"},{"location":"usage/#continuous-mode","text":"Run the AI without user authorization, 100% automated. Continuous mode is NOT recommended. It is potentially dangerous and may cause your AI to run forever or carry out actions you would not usually authorize. Use at your own risk. ./run.sh --continuous To exit the program, press Ctrl + C","title":"\ud83d\udc80 Continuous Mode \u26a0\ufe0f"},{"location":"usage/#self-feedback-mode","text":"Running Self-Feedback will INCREASE token use and thus cost more. This feature enables the agent to provide self-feedback by verifying its own actions and checking if they align with its current goals. If not, it will provide better feedback for the next loop. To enable this feature for the current loop, input S into the input field.","title":"\u267b\ufe0f Self-Feedback Mode \u26a0\ufe0f"},{"location":"usage/#gpt-35-only-mode","text":"If you don't have access to GPT-4, this mode allows you to use Mini-Boss! ./run.sh --gpt3only You can achieve the same by setting SMART_LLM_MODEL in .env to gpt-3.5-turbo .","title":"GPT-3.5 ONLY Mode"},{"location":"usage/#gpt-4-only-mode","text":"If you have access to GPT-4, this mode allows you to use Mini-Boss solely with GPT-4. This may give your bot increased intelligence. ./run.sh --gpt4only Warning Since GPT-4 is more expensive to use, running Mini-Boss in GPT-4-only mode will increase your API costs.","title":"GPT-4 ONLY Mode"},{"location":"usage/#logs","text":"Activity and error logs are located in the ./output/logs To print out debug logs: ./run.sh --debug","title":"Logs"},{"location":"configuration/memory/","text":"Setting Your Cache Type By default, Mini-Boss set up with Docker Compose will use Redis as its memory backend. Otherwise, the default is LocalCache (which stores memory in a JSON file). To switch to a different backend, change the MEMORY_BACKEND in .env to the value that you want: local uses a local JSON cache file pinecone uses the Pinecone.io account you configured in your ENV settings redis will use the redis cache that you configured milvus will use the milvus cache that you configured weaviate will use the weaviate cache that you configured Memory Backend Setup Links to memory backends Pinecone Milvus \u2013 self-hosted , or managed with Zilliz Cloud Redis Weaviate Redis Setup Important If you have set up Mini-Boss using Docker Compose, then Redis is included, no further setup needed. Caution This setup is not intended to be publicly accessible and lacks security measures. Avoid exposing Redis to the internet without a password or at all! Launch Redis container docker run -d --name redis-stack-server -p 6379 :6379 redis/redis-stack-server:latest Set the following settings in .env MEMORY_BACKEND = redis REDIS_HOST = localhost REDIS_PORT = 6379 REDIS_PASSWORD = <PASSWORD> Replace <PASSWORD> by your password, omitting the angled brackets (<>). Optional configuration: WIPE_REDIS_ON_START=False to persist memory stored in Redis between runs. MEMORY_INDEX=<WHATEVER> to specify a name for the memory index in Redis. The default is auto-gpt . Info See redis-stack-server for setting a password and additional configuration. \ud83c\udf32 Pinecone API Key Setup Pinecone lets you store vast amounts of vector-based memory, allowing the agent to load only relevant memories at any given time. Go to pinecone and make an account if you don't already have one. Choose the Starter plan to avoid being charged. Find your API key and region under the default project in the left sidebar. In the .env file set: PINECONE_API_KEY PINECONE_ENV (example: us-east4-gcp ) MEMORY_BACKEND=pinecone Milvus Setup Milvus is an open-source, highly scalable vector database to store huge amounts of vector-based memory and provide fast relevant search. It can be quickly deployed with docker, or as a cloud service provided by Zilliz Cloud . Deploy your Milvus service, either locally using docker or with a managed Zilliz Cloud database: Install and deploy Milvus locally Set up a managed Zilliz Cloud database Go to Zilliz Cloud and sign up if you don't already have account. In the Databases tab, create a new database. Remember your username and password Wait until the database status is changed to RUNNING. In the Database detail tab of the database you have created, the public cloud endpoint, such as: https://xxx-xxxx.xxxx.xxxx.zillizcloud.com:443 . Run pip3 install pymilvus to install the required client library. Make sure your PyMilvus version and Milvus version are compatible to avoid issues. See also the PyMilvus installation instructions . Update .env : MEMORY_BACKEND=milvus One of: MILVUS_ADDR=host:ip (for local instance) MILVUS_ADDR=https://xxx-xxxx.xxxx.xxxx.zillizcloud.com:443 (for Zilliz Cloud) The following settings are optional : MILVUS_USERNAME='username-of-your-milvus-instance' MILVUS_PASSWORD='password-of-your-milvus-instance' MILVUS_SECURE=True to use a secure connection. Only use if your Milvus instance has TLS enabled. Note: setting MILVUS_ADDR to a https:// URL will override this setting. MILVUS_COLLECTION to change the collection name to use in Milvus. Defaults to autogpt . Weaviate Setup Weaviate is an open-source vector database. It allows to store data objects and vector embeddings from ML-models and scales seamlessly to billion of data objects. To set up a Weaviate database, check out their Quickstart Tutorial . Although still experimental, Embedded Weaviate is supported which allows the Mini-Boss process itself to start a Weaviate instance. To enable it, set USE_WEAVIATE_EMBEDDED to True and make sure you pip install \"weaviate-client>=3.15.4\" . Install the Weaviate client Install the Weaviate client before usage. $ pip install weaviate-client Setting up environment variables In your .env file set the following: MEMORY_BACKEND = weaviate WEAVIATE_HOST = \"127.0.0.1\" # the IP or domain of the running Weaviate instance WEAVIATE_PORT = \"8080\" WEAVIATE_PROTOCOL = \"http\" WEAVIATE_USERNAME = \"your username\" WEAVIATE_PASSWORD = \"your password\" WEAVIATE_API_KEY = \"your weaviate API key if you have one\" WEAVIATE_EMBEDDED_PATH = \"/home/me/.local/share/weaviate\" # this is optional and indicates where the data should be persisted when running an embedded instance USE_WEAVIATE_EMBEDDED = False # set to True to run Embedded Weaviate MEMORY_INDEX = \"Autogpt\" # name of the index to create for the application View Memory Usage View memory usage by using the --debug flag :) \ud83e\udde0 Memory pre-seeding Memory pre-seeding allows you to ingest files into memory and pre-seed it before running Mini-Boss. $ python data_ingestion.py -h usage: data_ingestion.py [ -h ] ( --file FILE | --dir DIR ) [ --init ] [ --overlap OVERLAP ] [ --max_length MAX_LENGTH ] Ingest a file or a directory with multiple files into memory. Make sure to set your .env before running this script. options: -h, --help show this help message and exit --file FILE The file to ingest. --dir DIR The directory containing the files to ingest. --init Init the memory and wipe its content ( default: False ) --overlap OVERLAP The overlap size between chunks when ingesting files ( default: 200 ) --max_length MAX_LENGTH The max_length of each chunk when ingesting files ( default: 4000 ) # python data_ingestion.py --dir DataFolder --init --overlap 100 --max_length 2000 In the example above, the script initializes the memory, ingests all files within the Auto-Gpt/autogpt/auto_gpt_workspace/DataFolder directory into memory with an overlap between chunks of 100 and a maximum length of each chunk of 2000. Note that you can also use the --file argument to ingest a single file into memory and that data_ingestion.py will only ingest files within the /auto_gpt_workspace directory. The DIR path is relative to the auto_gpt_workspace directory, so python data_ingestion.py --dir . --init will ingest everything in auto_gpt_workspace directory. You can adjust the max_length and overlap parameters to fine-tune the way the documents are presented to the AI when it \"recall\" that memory: Adjusting the overlap value allows the AI to access more contextual information from each chunk when recalling information, but will result in more chunks being created and therefore increase memory backend usage and OpenAI API requests. Reducing the max_length value will create more chunks, which can save prompt tokens by allowing for more message history in the context, but will also increase the number of chunks. Increasing the max_length value will provide the AI with more contextual information from each chunk, reducing the number of chunks created and saving on OpenAI API requests. However, this may also use more prompt tokens and decrease the overall context available to the AI. Memory pre-seeding is a technique for improving AI accuracy by ingesting relevant data into its memory. Chunks of data are split and added to memory, allowing the AI to access them quickly and generate more accurate responses. It's useful for large datasets or when specific information needs to be accessed quickly. Examples include ingesting API or GitHub documentation before running Mini-Boss. Attention If you use Redis for memory, make sure to run Mini-Boss with WIPE_REDIS_ON_START=False For other memory backends, we currently forcefully wipe the memory when starting Mini-Boss. To ingest data with those memory backends, you can call the data_ingestion.py script anytime during an Mini-Boss run. Memories will be available to the AI immediately as they are ingested, even if ingested while Mini-Boss is running.","title":"Memory"},{"location":"configuration/memory/#setting-your-cache-type","text":"By default, Mini-Boss set up with Docker Compose will use Redis as its memory backend. Otherwise, the default is LocalCache (which stores memory in a JSON file). To switch to a different backend, change the MEMORY_BACKEND in .env to the value that you want: local uses a local JSON cache file pinecone uses the Pinecone.io account you configured in your ENV settings redis will use the redis cache that you configured milvus will use the milvus cache that you configured weaviate will use the weaviate cache that you configured","title":"Setting Your Cache Type"},{"location":"configuration/memory/#memory-backend-setup","text":"Links to memory backends Pinecone Milvus \u2013 self-hosted , or managed with Zilliz Cloud Redis Weaviate","title":"Memory Backend Setup"},{"location":"configuration/memory/#redis-setup","text":"Important If you have set up Mini-Boss using Docker Compose, then Redis is included, no further setup needed. Caution This setup is not intended to be publicly accessible and lacks security measures. Avoid exposing Redis to the internet without a password or at all! Launch Redis container docker run -d --name redis-stack-server -p 6379 :6379 redis/redis-stack-server:latest Set the following settings in .env MEMORY_BACKEND = redis REDIS_HOST = localhost REDIS_PORT = 6379 REDIS_PASSWORD = <PASSWORD> Replace <PASSWORD> by your password, omitting the angled brackets (<>). Optional configuration: WIPE_REDIS_ON_START=False to persist memory stored in Redis between runs. MEMORY_INDEX=<WHATEVER> to specify a name for the memory index in Redis. The default is auto-gpt . Info See redis-stack-server for setting a password and additional configuration.","title":"Redis Setup"},{"location":"configuration/memory/#pinecone-api-key-setup","text":"Pinecone lets you store vast amounts of vector-based memory, allowing the agent to load only relevant memories at any given time. Go to pinecone and make an account if you don't already have one. Choose the Starter plan to avoid being charged. Find your API key and region under the default project in the left sidebar. In the .env file set: PINECONE_API_KEY PINECONE_ENV (example: us-east4-gcp ) MEMORY_BACKEND=pinecone","title":"\ud83c\udf32 Pinecone API Key Setup"},{"location":"configuration/memory/#milvus-setup","text":"Milvus is an open-source, highly scalable vector database to store huge amounts of vector-based memory and provide fast relevant search. It can be quickly deployed with docker, or as a cloud service provided by Zilliz Cloud . Deploy your Milvus service, either locally using docker or with a managed Zilliz Cloud database: Install and deploy Milvus locally Set up a managed Zilliz Cloud database Go to Zilliz Cloud and sign up if you don't already have account. In the Databases tab, create a new database. Remember your username and password Wait until the database status is changed to RUNNING. In the Database detail tab of the database you have created, the public cloud endpoint, such as: https://xxx-xxxx.xxxx.xxxx.zillizcloud.com:443 . Run pip3 install pymilvus to install the required client library. Make sure your PyMilvus version and Milvus version are compatible to avoid issues. See also the PyMilvus installation instructions . Update .env : MEMORY_BACKEND=milvus One of: MILVUS_ADDR=host:ip (for local instance) MILVUS_ADDR=https://xxx-xxxx.xxxx.xxxx.zillizcloud.com:443 (for Zilliz Cloud) The following settings are optional : MILVUS_USERNAME='username-of-your-milvus-instance' MILVUS_PASSWORD='password-of-your-milvus-instance' MILVUS_SECURE=True to use a secure connection. Only use if your Milvus instance has TLS enabled. Note: setting MILVUS_ADDR to a https:// URL will override this setting. MILVUS_COLLECTION to change the collection name to use in Milvus. Defaults to autogpt .","title":"Milvus Setup"},{"location":"configuration/memory/#weaviate-setup","text":"Weaviate is an open-source vector database. It allows to store data objects and vector embeddings from ML-models and scales seamlessly to billion of data objects. To set up a Weaviate database, check out their Quickstart Tutorial . Although still experimental, Embedded Weaviate is supported which allows the Mini-Boss process itself to start a Weaviate instance. To enable it, set USE_WEAVIATE_EMBEDDED to True and make sure you pip install \"weaviate-client>=3.15.4\" .","title":"Weaviate Setup"},{"location":"configuration/memory/#install-the-weaviate-client","text":"Install the Weaviate client before usage. $ pip install weaviate-client","title":"Install the Weaviate client"},{"location":"configuration/memory/#setting-up-environment-variables","text":"In your .env file set the following: MEMORY_BACKEND = weaviate WEAVIATE_HOST = \"127.0.0.1\" # the IP or domain of the running Weaviate instance WEAVIATE_PORT = \"8080\" WEAVIATE_PROTOCOL = \"http\" WEAVIATE_USERNAME = \"your username\" WEAVIATE_PASSWORD = \"your password\" WEAVIATE_API_KEY = \"your weaviate API key if you have one\" WEAVIATE_EMBEDDED_PATH = \"/home/me/.local/share/weaviate\" # this is optional and indicates where the data should be persisted when running an embedded instance USE_WEAVIATE_EMBEDDED = False # set to True to run Embedded Weaviate MEMORY_INDEX = \"Autogpt\" # name of the index to create for the application","title":"Setting up environment variables"},{"location":"configuration/memory/#view-memory-usage","text":"View memory usage by using the --debug flag :)","title":"View Memory Usage"},{"location":"configuration/memory/#memory-pre-seeding","text":"Memory pre-seeding allows you to ingest files into memory and pre-seed it before running Mini-Boss. $ python data_ingestion.py -h usage: data_ingestion.py [ -h ] ( --file FILE | --dir DIR ) [ --init ] [ --overlap OVERLAP ] [ --max_length MAX_LENGTH ] Ingest a file or a directory with multiple files into memory. Make sure to set your .env before running this script. options: -h, --help show this help message and exit --file FILE The file to ingest. --dir DIR The directory containing the files to ingest. --init Init the memory and wipe its content ( default: False ) --overlap OVERLAP The overlap size between chunks when ingesting files ( default: 200 ) --max_length MAX_LENGTH The max_length of each chunk when ingesting files ( default: 4000 ) # python data_ingestion.py --dir DataFolder --init --overlap 100 --max_length 2000 In the example above, the script initializes the memory, ingests all files within the Auto-Gpt/autogpt/auto_gpt_workspace/DataFolder directory into memory with an overlap between chunks of 100 and a maximum length of each chunk of 2000. Note that you can also use the --file argument to ingest a single file into memory and that data_ingestion.py will only ingest files within the /auto_gpt_workspace directory. The DIR path is relative to the auto_gpt_workspace directory, so python data_ingestion.py --dir . --init will ingest everything in auto_gpt_workspace directory. You can adjust the max_length and overlap parameters to fine-tune the way the documents are presented to the AI when it \"recall\" that memory: Adjusting the overlap value allows the AI to access more contextual information from each chunk when recalling information, but will result in more chunks being created and therefore increase memory backend usage and OpenAI API requests. Reducing the max_length value will create more chunks, which can save prompt tokens by allowing for more message history in the context, but will also increase the number of chunks. Increasing the max_length value will provide the AI with more contextual information from each chunk, reducing the number of chunks created and saving on OpenAI API requests. However, this may also use more prompt tokens and decrease the overall context available to the AI. Memory pre-seeding is a technique for improving AI accuracy by ingesting relevant data into its memory. Chunks of data are split and added to memory, allowing the AI to access them quickly and generate more accurate responses. It's useful for large datasets or when specific information needs to be accessed quickly. Examples include ingesting API or GitHub documentation before running Mini-Boss. Attention If you use Redis for memory, make sure to run Mini-Boss with WIPE_REDIS_ON_START=False For other memory backends, we currently forcefully wipe the memory when starting Mini-Boss. To ingest data with those memory backends, you can call the data_ingestion.py script anytime during an Mini-Boss run. Memories will be available to the AI immediately as they are ingested, even if ingested while Mini-Boss is running.","title":"\ud83e\udde0 Memory pre-seeding"}]}